{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c88f42-6d43-44da-adcc-53286d21bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  \n",
    "from abc import abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from DDIM_C import GaussianDiffusion\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d24f3d-7e27-49af-9d70-9382962be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : \n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n",
    "])\n",
    "\n",
    "class PalmDataset(Dataset):\n",
    "    def __init__(self, palm_dir, label_dir, transform=None):\n",
    "        self.palm_dir = palm_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.palm_images = sorted([f for f in os.listdir(palm_dir) \n",
    "                                if os.path.isfile(os.path.join(palm_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.label_images = sorted([f for f in os.listdir(label_dir) \n",
    "                                    if os.path.isfile(os.path.join(label_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.palm_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        palm_path = os.path.join(self.palm_dir, self.palm_images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_images[idx])\n",
    "        palm_image = Image.open(palm_path).convert(\"RGB\")\n",
    "        label_image = Image.open(label_path).convert(\"RGB\")\n",
    "        \n",
    "        palm_image = palm_image.resize((256, 256))\n",
    "        label_image = label_image.resize((64, 64))\n",
    "\n",
    "        if self.transform:\n",
    "            palm_image = self.transform(palm_image)\n",
    "            label_image = self.transform(label_image)\n",
    "\n",
    "        return palm_image, label_image\n",
    "\n",
    "\n",
    "\n",
    "real_image_folder = './datasets/palm2bezier/train/trainA'\n",
    "bezier_image_folder='./datasets/palm2bezier/test/binary_images_fake_bezier'\n",
    "\n",
    "dataset = PalmDataset(real_image_folder, bezier_image_folder, transform=transform2)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    palm_images = torch.stack([item[0] for item in batch])\n",
    "    label_images = torch.stack([item[1] for item in batch])\n",
    "    return palm_images, label_images\n",
    "\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          num_workers=16, \n",
    "                          pin_memory=True, \n",
    "                          collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef224b-2c04-40e5-8b0a-14457938fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VQModel(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quantize): VectorQuantizer2(\n",
       "    (embedding): Embedding(8192, 3)\n",
       "  )\n",
       "  (quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2 : \n",
    "ckpt_path = 'vqmodel_checkpoint.ckpt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "\n",
    "class VQModel(torch.nn.Module):\n",
    "    def __init__(self, ddconfig, embed_dim=3,n_embed=8192):\n",
    "        super().__init__()\n",
    "        from taming.modules.diffusionmodules.model import Encoder, Decoder\n",
    "        # from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        # self.decoder = Decoder(**ddconfig)\n",
    "        # self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
    "        #                                 remap=None,\n",
    "        #                                 sane_index_shape=False)\n",
    "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        # self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "\n",
    "    def encode(self, x):  \n",
    "        h = self.quant_conv(self.encoder(x))\n",
    "        return h\n",
    "\n",
    "    # def decode(self, x, force_not_quantize=False):  \n",
    "    #     if not force_not_quantize:\n",
    "    #         quant, emb_loss, info = self.quantize(x)\n",
    "    #     else:\n",
    "    #         quant = x\n",
    "    #     dec = self.decoder(self.post_quant_conv(quant))\n",
    "    #     return dec\n",
    "\n",
    "\n",
    "vq_model = VQModel(  \n",
    "    ddconfig={\n",
    "        'double_z': False,\n",
    "        'z_channels': 3,\n",
    "        'resolution': 256,\n",
    "        'in_channels': 3,\n",
    "        'out_ch': 3,\n",
    "        'ch': 128,\n",
    "        'ch_mult': [1, 2, 4],\n",
    "        'num_res_blocks': 2,\n",
    "        'attn_resolutions': [],\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    embed_dim=3,\n",
    "    n_embed=8192\n",
    ")\n",
    "\n",
    "\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "elif 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint  \n",
    "\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in vq_model.state_dict()}\n",
    "vq_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "vq_model = vq_model.to(device)\n",
    "for param in vq_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vq_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47aa30-515d-4d4a-af43-54759728079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=224, out_features=896, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=896, out_features=896, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(6, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(224, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(224, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=448, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(224, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(448, 1344, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(448, 448, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=448, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(448, 1344, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(448, 448, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(448, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(448, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=672, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(448, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(672, 2016, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(672, 672, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=672, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(672, 2016, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(672, 672, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(672, 672, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(672, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(672, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(896, 2688, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (11): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(896, 2688, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "      (qkv): Conv1d(896, 2688, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttentionLegacy()\n",
       "      (proj_out): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1792, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1792, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1792, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(896, 2688, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1568, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1568, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=896, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1568, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(896, 2688, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1568, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1568, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=672, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1568, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(672, 2016, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(672, 672, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1344, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1344, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=672, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1344, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(672, 2016, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(672, 672, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1120, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1120, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=672, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1120, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(672, 2016, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(672, 672, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1120, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1120, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=448, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1120, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(448, 1344, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(448, 448, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 896, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(896, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=448, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(896, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(448, 1344, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(448, 448, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(672, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=448, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(672, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(448, 1344, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(448, 448, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 672, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(672, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(672, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 448, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(448, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 224, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(224, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3 : \n",
    "unet_config = {\n",
    "    \"image_size\": 64,\n",
    "    \"in_channels\": 6,\n",
    "    \"out_channels\": 3,\n",
    "    \"model_channels\": 224,\n",
    "    \"attention_resolutions\": [8, 4, 2],\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"channel_mult\": [1, 2, 3, 4],\n",
    "    \"num_head_channels\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "unet_model = UNetModel(**unet_config)\n",
    "\n",
    "\n",
    "checkpoint_path = 'ddim_c.ckpt'  \n",
    "\n",
    "\n",
    "pretrained_dict = torch.load(checkpoint_path,weights_only = True)\n",
    "\n",
    "\n",
    "unet_model.load_state_dict(pretrained_dict)\n",
    "\n",
    "print(\"Pretrained weights loaded successfully.\")\n",
    "\n",
    "unet_model.to(device)\n",
    "unet_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf75188-d128-4f86-b261-1ecf5d0265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 : \n",
    "timesteps = 1000\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=timesteps,\n",
    "        beta_schedule='linear',\n",
    "        linear_start = 0.0015,\n",
    "        linear_end= 0.0155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ae962-04af-4fcb-a1df-2bad88e35926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [04:06<6:46:29, 246.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [08:11<6:41:33, 245.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.3956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 3/100 [12:17<6:37:16, 245.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 4/100 [16:23<6:33:09, 245.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 5/100 [20:28<6:29:06, 245.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 6/100 [24:34<6:24:56, 245.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.2835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 7/100 [28:40<6:20:49, 245.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 8/100 [32:45<6:16:34, 245.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 9/100 [36:51<6:12:26, 245.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.1427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 10/100 [40:56<6:08:22, 245.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 11/100 [45:02<6:04:18, 245.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 12/100 [49:07<6:00:06, 245.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 13/100 [53:13<5:56:07, 245.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 15/100 [1:01:25<5:48:05, 245.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 16/100 [1:05:30<5:43:56, 245.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|█▊        | 18/100 [1:13:42<5:35:47, 245.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▉        | 19/100 [1:17:48<5:31:44, 245.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.1295\n",
      "Epoch [20/100], Loss: 0.1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 20/100 [1:21:55<5:28:26, 246.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved to ddim_c.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██        | 21/100 [1:26:01<5:24:03, 246.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 22/100 [1:30:07<5:19:47, 246.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.1423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 23/100 [1:34:12<5:15:34, 245.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 24/100 [1:38:18<5:11:26, 245.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 25/100 [1:42:24<5:07:20, 245.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 26/100 [1:46:30<5:03:15, 245.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 27/100 [1:50:35<4:59:00, 245.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 28/100 [1:54:41<4:54:52, 245.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.1809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▉       | 29/100 [1:58:47<4:50:46, 245.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.2182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 30/100 [2:02:52<4:46:39, 245.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███       | 31/100 [2:06:58<4:42:37, 245.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 32/100 [2:11:04<4:38:35, 245.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 33/100 [2:15:10<4:34:30, 245.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 34/100 [2:19:16<4:30:19, 245.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 35/100 [2:23:21<4:26:08, 245.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 36/100 [2:27:27<4:22:02, 245.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 37/100 [2:31:32<4:17:56, 245.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 38/100 [2:35:38<4:13:55, 245.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▉      | 39/100 [2:39:44<4:09:47, 245.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.1867\n",
      "Epoch [40/100], Loss: 0.1599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 40/100 [2:43:52<4:06:23, 246.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved to ddim_c.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████      | 41/100 [2:47:57<4:02:00, 246.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 0.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 42/100 [2:52:03<3:57:40, 245.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 43/100 [2:56:08<3:53:30, 245.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 0.1295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 44/100 [3:00:14<3:49:17, 245.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 0.1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 45/100 [3:04:19<3:45:10, 245.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Loss: 0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 46/100 [3:08:25<3:41:02, 245.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Loss: 0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 47/100 [3:12:30<3:36:55, 245.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Loss: 0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 48/100 [3:16:36<3:32:47, 245.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Loss: 0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▉     | 49/100 [3:20:41<3:28:42, 245.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Loss: 0.1448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████     | 51/100 [3:28:53<3:20:33, 245.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Loss: 0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 52/100 [3:32:58<3:16:24, 245.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Loss: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 53/100 [3:37:04<3:12:21, 245.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Loss: 0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 54/100 [3:41:09<3:08:16, 245.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Loss: 0.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 55/100 [3:45:15<3:04:12, 245.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Loss: 0.1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 56/100 [3:49:20<3:00:04, 245.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Loss: 0.2019\n"
     ]
    }
   ],
   "source": [
    "#Step 5 : \n",
    "optimizer = torch.optim.AdamW(unet_model.parameters(), lr=1e-06)\n",
    "\n",
    "\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-7)\n",
    "\n",
    "global_step = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    for palm_images, label_images in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        palm_images = palm_images.to(device)\n",
    "        label_images = label_images.to(device)\n",
    "        \n",
    "        batch_size = palm_images.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latent = vq_model.encode(palm_images).detach()\n",
    "            \n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        loss = gaussian_diffusion.train_losses(unet_model, latent, t, label_images)\n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "            \n",
    "        global_step += 1\n",
    "    \n",
    "    \n",
    "    cosine_scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(unet_model.state_dict(), 'ddim_c.ckpt')\n",
    "        print(\"Model parameters saved to ddim_c.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

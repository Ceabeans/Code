{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8937e5d8-fa63-4479-a513-8c08679590f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # 导入 tqdm 库\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from typing import List\n",
    "from dataclasses import asdict, dataclass,field\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor, nn\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from consistency_models.utils import update_ema_model_\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
    "from consistency_models import ConsistencySamplingAndEditing, ImprovedConsistencyTraining, pseudo_huber_loss\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f332045-a1d5-4424-a4ab-beb92e76c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5713/519805230.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VQModel(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型参数\n",
    "ckpt_path = 'vqmodel_checkpoint.ckpt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "# 简化版VQModel的类\n",
    "class VQModel(torch.nn.Module):\n",
    "    def __init__(self, ddconfig, embed_dim):\n",
    "        super().__init__()\n",
    "        from taming.modules.diffusionmodules.model import Encoder, Decoder\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "\n",
    "    def encode(self, x):  # 修改方法名称，避免与属性名冲突\n",
    "        h = self.quant_conv(self.encoder(x))\n",
    "        return h\n",
    "\n",
    "    def decode(self, x):  # 修改方法名称，避免与属性名冲突\n",
    "        dec = self.decoder(self.post_quant_conv(x))\n",
    "        return dec\n",
    "        \n",
    "# 初始化简化版模型\n",
    "vq_model = VQModel(  # 实例化对象的名称改为小写以避免与类名混淆\n",
    "    ddconfig={\n",
    "        'double_z': False,\n",
    "        'z_channels': 3,\n",
    "        'resolution': 256,\n",
    "        'in_channels': 3,\n",
    "        'out_ch': 3,\n",
    "        'ch': 128,\n",
    "        'ch_mult': [1, 2, 4],\n",
    "        'num_res_blocks': 2,\n",
    "        'attn_resolutions': [],\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    embed_dim=3\n",
    ")\n",
    "\n",
    "# 加载权重\n",
    "# 过滤掉与模型不相关的参数\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "elif 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint  # 如果checkpoint文件本身就是状态字典\n",
    "\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in vq_model.state_dict()}\n",
    "vq_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "vq_model = vq_model.to(device)\n",
    "for param in vq_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 设置模型为评估模式   \n",
    "vq_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c257cf07-7819-40e3-9d14-af0e21edff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LitImprovedConsistencyModelConfig:\n",
    "    ema_decay_rate: float = 0.0\n",
    "    lr: float = 1e-4\n",
    "    betas: Tuple[float, float] = (0.9, 0.995)\n",
    "    lr_scheduler_start_factor: float = 1e-5\n",
    "    lr_scheduler_iters: int = 1_000\n",
    "    sample_every_n_steps: int = 1_000\n",
    "    num_samples: int = 8\n",
    "    sampling_sigmas: Tuple[Tuple[int, ...], ...] = (\n",
    "        (80,),\n",
    "        (80.0, 0.661),\n",
    "        (80.0, 24.4, 5.84, 0.9, 0.661),\n",
    "    )\n",
    "\n",
    "class LitImprovedConsistencyModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        consistency_training: ImprovedConsistencyTraining,\n",
    "        consistency_sampling: ConsistencySamplingAndEditing,\n",
    "        model: UNetModel,\n",
    "        ema_model: UNetModel,\n",
    "        config: LitImprovedConsistencyModelConfig,\n",
    "        vq_model: nn.Module ,\n",
    "    ) -> None:\n",
    "        super().__init__()                                # 初始化父类 LightningModule\n",
    "\n",
    "        self.consistency_training = consistency_training  # 保存一致性训练模块\n",
    "        self.consistency_sampling = consistency_sampling  # 保存一致性采样模块\n",
    "        self.model = model                                # 保存主模型\n",
    "        self.ema_model = ema_model                        # 保存 EMA 模型\n",
    "        self.config = config                              # 保存配置类实例\n",
    "        self.vq_model = vq_model\n",
    "\n",
    "        # Freeze the EMA model and set it to eval mode\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.requires_grad = False                   # 冻结 EMA 模型的参数，不更新梯度\n",
    "        self.ema_model = self.ema_model.eval()            # 设置 EMA 模型为评估模式\n",
    "\n",
    "        for param in self.vq_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # 设置模型为评估模式   \n",
    "        self.vq_model = self.vq_model.eval()\n",
    "\n",
    "    def training_step(self, batch: Union[Tensor, List[Tensor]], batch_idx: int) -> None:\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]                              # 如果 batch 是列表，则取第一个张量\n",
    "        batch = self.vq_model.encode(batch)\n",
    "        output = self.consistency_training(\n",
    "            self.model, batch, self.global_step, self.trainer.max_steps\n",
    "        )                                                 # 调用一致性训练模块，获取模型输出和目标输出\n",
    "\n",
    "        loss = (\n",
    "            pseudo_huber_loss(output.predicted, output.target) * output.loss_weights\n",
    "        ).mean()                                          # 计算伪 Huber 损失，并使用输出中的损失权重进行加权\n",
    "\n",
    "        # self.log_dict({\"train_loss\": loss, \"num_timesteps\": output.num_timesteps})  \n",
    "        #                                                   # 记录训练损失和时间步长\n",
    "\n",
    "        return loss                                       # 返回计算的损失\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self, outputs: Any, batch: Union[Tensor, List[Tensor]], batch_idx: int\n",
    "    ) -> None:\n",
    "        update_ema_model_(self.model, self.ema_model, self.config.ema_decay_rate)  \n",
    "                                                          # 更新 EMA 模型的参数\n",
    "\n",
    "        if (\n",
    "            (self.global_step + 1) % self.config.sample_every_n_steps == 0\n",
    "        ) or self.global_step == 0:\n",
    "            self.__sample_and_log_samples(batch)          # 如果达到采样间隔，进行采样并记录样本\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.config.lr, betas=self.config.betas\n",
    "        )                                                 # 使用 AdamW 优化器，设置学习率和动量参数\n",
    "        sched = torch.optim.lr_scheduler.LinearLR(\n",
    "            opt,\n",
    "            start_factor=self.config.lr_scheduler_start_factor,\n",
    "            total_iters=self.config.lr_scheduler_iters,\n",
    "        )                                                 # 使用线性学习率调度器，设置学习率的起始因子和总迭代次数\n",
    "        sched = {\"scheduler\": sched, \"interval\": \"step\", \"frequency\": 1}  # 定义调度器的更新频率\n",
    "\n",
    "        return [opt], [sched]  # 返回优化器和调度器\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __sample_and_log_samples(self, batch: Union[Tensor, List[Tensor]]) -> None:\n",
    "        if isinstance(batch, list):\n",
    "            batch = batch[0]  # 如果 batch 是列表，则取第一个张量\n",
    "\n",
    "        # Ensure the number of samples does not exceed the batch size\n",
    "        num_samples = min(self.config.num_samples, batch.shape[0])  # 确保采样数量不超过批次大小\n",
    "\n",
    "        # Log ground truth samples\n",
    "        self.__log_images(\n",
    "            batch[:num_samples].detach().clone(), f\"ground_truth\", self.global_step\n",
    "        )  # 记录实际的（ground truth）样本\n",
    "\n",
    "        latent_batch = self.vq_model.encode(batch[:num_samples].detach().clone()).detach()\n",
    "        for sigmas in self.config.sampling_sigmas:\n",
    "            samples = self.consistency_sampling(\n",
    "                self.ema_model, latent_batch, sigmas, verbose=True\n",
    "            )  # 使用 EMA 模型和噪声生成样本\n",
    "            samples = self.vq_model.decode(samples)\n",
    "\n",
    "            # Generated samples\n",
    "            self.__log_images(\n",
    "                samples,\n",
    "                f\"generated_samples-sigmas={sigmas}\",\n",
    "                self.global_step,\n",
    "            )  # 记录生成的样本\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __log_images(self, images: Tensor, title: str, global_step: int) -> None:\n",
    "        images = images.detach().float()  # 确保图像数据是浮点数并分离计算图\n",
    "\n",
    "        grid = make_grid(\n",
    "            images.clamp(-1.0, 1.0), value_range=(-1.0, 1.0), normalize=True\n",
    "        )  # 将图像拼接成网格，并进行归一化处理\n",
    "        self.logger.experiment.add_image(title, grid, global_step)  # 将图像添加到日志中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf53ecc-8650-4c8b-a66d-01e639f82605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 将像素值限制在-1到1之间\n",
    "])\n",
    "\n",
    "# 自定义数据集\n",
    "class RealPalmDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for subfolder2 in os.listdir(root_dir):\n",
    "            subfolder2_path = os.path.join(root_dir, subfolder2)\n",
    "            if os.path.isdir(subfolder2_path):\n",
    "                for filenameB in os.listdir(subfolder2_path):\n",
    "                    image_path = os.path.join(subfolder2_path, filenameB)\n",
    "                    if os.path.isfile(image_path):\n",
    "                        self.image_paths.append(image_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 将图像转为RGB模式\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# 定义real_image_folder路径\n",
    "real_image_folder = '/root/onethingai-fs/realpalm_200x40'\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset_real_palm_B = RealPalmDataset(real_image_folder, transform=transform2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1443a5c5-9cc6-4ed6-bc4f-b1eb922e2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UNetConfig:\n",
    "    image_size: int = 64\n",
    "    in_channels: int = 3\n",
    "    out_channels: int = 3\n",
    "    model_channels: int = 128\n",
    "    attention_resolutions: List[int] = field(default_factory=lambda: [])  # 添加这个参数\n",
    "    num_res_blocks: int = 2\n",
    "    channel_mult: List[int] = field(default_factory=lambda: [1, 2, 4, 8])\n",
    "    num_heads: int = 8\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    unet_config: UNetConfig\n",
    "    consistency_training: ImprovedConsistencyTraining\n",
    "    consistency_sampling: ConsistencySamplingAndEditing\n",
    "    lit_icm_config: LitImprovedConsistencyModelConfig\n",
    "    trainer: Trainer\n",
    "    seed: int = 42\n",
    "    resume_ckpt_path: Optional[str] = None\n",
    "\n",
    "\n",
    "def run_training(config: TrainingConfig) -> None:\n",
    "    # Set seed\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    # Create data module\n",
    "    dm = DataLoader(\n",
    "        dataset_real_palm_B, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=8, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create model and its EMA\n",
    "    model = UNetModel(\n",
    "        image_size=config.unet_config.image_size,\n",
    "        in_channels=config.unet_config.in_channels,\n",
    "        out_channels=config.unet_config.out_channels,\n",
    "        model_channels=config.unet_config.model_channels,\n",
    "        attention_resolutions=config.unet_config.attention_resolutions,\n",
    "        num_res_blocks=config.unet_config.num_res_blocks,\n",
    "        channel_mult=config.unet_config.channel_mult,\n",
    "        num_heads=config.unet_config.num_heads\n",
    "    )\n",
    "\n",
    "    ema_model = UNetModel(\n",
    "        image_size=config.unet_config.image_size,\n",
    "        in_channels=config.unet_config.in_channels,\n",
    "        out_channels=config.unet_config.out_channels,\n",
    "        model_channels=config.unet_config.model_channels,\n",
    "        attention_resolutions=config.unet_config.attention_resolutions,\n",
    "        num_res_blocks=config.unet_config.num_res_blocks,\n",
    "        channel_mult=config.unet_config.channel_mult,\n",
    "        num_heads=config.unet_config.num_heads\n",
    "    )\n",
    "    ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Create lightning module\n",
    "    lit_icm = LitImprovedConsistencyModel(\n",
    "        config.consistency_training,\n",
    "        config.consistency_sampling,\n",
    "        model,\n",
    "        ema_model,\n",
    "        config.lit_icm_config,\n",
    "        vq_model\n",
    "    )\n",
    "\n",
    "    # Run training\n",
    "    config.trainer.fit(lit_icm, dm, ckpt_path=config.resume_ckpt_path)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(lit_icm.model.state_dict(), 'cm.ckpt')\n",
    "    print(\"Model parameters saved to cm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5f748-8dbe-461a-9efa-57b281d0941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory ./logs/icm/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model     | UNetModel | 226 M  | train\n",
      "1 | ema_model | UNetModel | 226 M  | eval \n",
      "2 | vq_model  | VQModel   | 55.3 M | eval \n",
      "------------------------------------------------\n",
      "226 M     Trainable params\n",
      "281 M     Non-trainable params\n",
      "507 M     Total params\n",
      "2,030.652 Total estimated model params size (MB)\n",
      "384       Modules in train mode\n",
      "555       Modules in eval mode\n",
      "/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (500) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████▉| 499/500 [02:24<00:00,  3.45it/s, v_num=icm]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=0.6610): 100%|██████████| 1/1 [00:00<00:00, 46.76it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=24.4000):   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=5.8400):   0%|          | 0/4 [00:00<?, ?it/s] \u001b[A\n",
      "sampling (σ=0.9000):   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=0.6610): 100%|██████████| 4/4 [00:00<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████▉| 499/500 [02:25<00:00,  3.44it/s, v_num=icm]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=0.6610): 100%|██████████| 1/1 [00:00<00:00, 45.51it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=24.4000):   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=5.8400):   0%|          | 0/4 [00:00<?, ?it/s] \u001b[A\n",
      "sampling (σ=0.9000):   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "sampling (σ=0.6610): 100%|██████████| 4/4 [00:00<00:00, 42.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 500/500 [02:25<00:00,  3.44it/s, v_num=icm]"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    unet_config = UNetConfig(),\n",
    "    consistency_training = ImprovedConsistencyTraining(),\n",
    "    consistency_sampling = ConsistencySamplingAndEditing(),\n",
    "    lit_icm_config = LitImprovedConsistencyModelConfig(\n",
    "        sample_every_n_steps=1000, lr_scheduler_iters=1000\n",
    "    ),\n",
    "    trainer=Trainer(\n",
    "        max_steps=10_000,\n",
    "        precision=32,\n",
    "        log_every_n_steps=1000,\n",
    "        logger=TensorBoardLogger(\".\", name=\"logs\", version=\"icm\"),\n",
    "        callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    "    ),\n",
    ")\n",
    "run_training(training_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

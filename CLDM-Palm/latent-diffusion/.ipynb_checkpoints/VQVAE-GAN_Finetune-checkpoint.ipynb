{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137921ef-edc9-4352-8b76-33f931829a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm  # 导入 tqdm 库\n",
    "\n",
    "from taming.models.vqgan import VQModel\n",
    "from taming.modules.losses.vqperceptual import VQLPIPSWithDiscriminator\n",
    "\n",
    "# 配置文件\n",
    "config = {\n",
    "    'base_learning_rate': 4.5e-7,\n",
    "    'target': 'taming.models.vqgan.VQModel',\n",
    "    'params': {\n",
    "        'embed_dim': 256,\n",
    "        'n_embed': 1024,\n",
    "        'ddconfig': {\n",
    "            'double_z': False,\n",
    "            'z_channels': 256,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 128,\n",
    "            'ch_mult': [1, 1, 2, 2, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [16],\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'lossconfig': {\n",
    "            'target': 'taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator',\n",
    "            'params': {\n",
    "                'disc_conditional': False,\n",
    "                'disc_in_channels': 3,\n",
    "                'disc_start': 30001,\n",
    "                'disc_weight': 0.8,\n",
    "                'codebook_weight': 1.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f7b21f-28c1-4c5e-8b6f-14aa3a9e1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 将像素值限制在-1到1之间\n",
    "])\n",
    "\n",
    "# 自定义数据集\n",
    "class RealPalmDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for subfolder2 in os.listdir(root_dir):\n",
    "            subfolder2_path = os.path.join(root_dir, subfolder2)\n",
    "            if os.path.isdir(subfolder2_path):\n",
    "                for filenameB in os.listdir(subfolder2_path):\n",
    "                    image_path = os.path.join(subfolder2_path, filenameB)\n",
    "                    if os.path.isfile(image_path):\n",
    "                        self.image_paths.append(image_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 将图像转为RGB模式\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# 定义real_image_folder路径\n",
    "real_image_folder = '/root/onethingai-fs/realpalm_200x40'\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset_real_palm_B = RealPalmDataset(real_image_folder, transform=transform2)\n",
    "\n",
    "train_loader = DataLoader(dataset_real_palm_B, \n",
    "                          batch_size=8, \n",
    "                          shuffle=True, \n",
    "                          num_workers=8, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eefec05-be23-4d40-a430-ccad457effc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.5, 0.5, 0.5]).view(1, 3, 1, 1).to(device)\n",
    "std = torch.tensor([0.5, 0.5, 0.5]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "# 创建保存路径\n",
    "save_dir1 = os.path.join('/root/onethingai-tmp/taming-transformers-master', 'input_fine')\n",
    "save_dir2 = os.path.join('/root/onethingai-tmp/taming-transformers-master', 'output_fine')\n",
    "\n",
    "os.makedirs(save_dir1, exist_ok=True)\n",
    "os.makedirs(save_dir2, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3832422-97f9-417d-9779-54560038b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "model = VQModel(**config['params'])\n",
    "checkpoint = torch.load('last.ckpt', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# 设置损失函数\n",
    "loss_fn = VQLPIPSWithDiscriminator(**config['params']['lossconfig']['params'])\n",
    "loss_fn.to(device)  # 将损失函数也移动到设备上\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['base_learning_rate'])\n",
    "\n",
    "# 训练循环\n",
    "def train(model, dataloader, loss_fn, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructions, codebook_loss = model(images)\n",
    "            last_layer = model.get_last_layer()  # 获取最后一层权重\n",
    "            loss, _ = loss_fn(codebook_loss=codebook_loss, \n",
    "                              inputs=images, \n",
    "                              reconstructions=reconstructions, \n",
    "                              optimizer_idx=0,  # 始终更新生成器\n",
    "                              global_step=global_step, \n",
    "                              last_layer=last_layer)  # 传递 last_layer\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            global_step += 1\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # 在指定的epochs保存图片\n",
    "        if epoch in [4, 9, 14,19]:  # 由于epoch从0开始计数\n",
    "            for i, (input_i, output_i) in enumerate(zip(images, reconstructions)):\n",
    "                # 反归一化\n",
    "                input_i = input_i * std + mean\n",
    "                output_i = output_i * std + mean\n",
    "\n",
    "                input = transforms.ToPILImage()(input_i.cpu().squeeze())\n",
    "                output = transforms.ToPILImage()(output_i.cpu().squeeze())\n",
    "\n",
    "                # 在文件名中包含epoch信息以避免覆盖\n",
    "                name_B1 = f'input{epoch+1}_pair{i+1}.png'\n",
    "                name_B2 = f'output{epoch+1}_pair{i+1}.png'\n",
    "\n",
    "                save_path1 = os.path.join(save_dir1, name_B1)\n",
    "                save_path2 = os.path.join(save_dir2, name_B2)\n",
    "\n",
    "                input.save(save_path1)\n",
    "                output.save(save_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1812be1-ad56-4e94-bf24-bf44e5e41e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 1/20 [06:55<2:11:37, 415.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.3748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 2/20 [13:54<2:05:11, 417.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.3495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 3/20 [20:54<1:58:35, 418.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.3433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 4/20 [27:53<1:51:40, 418.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.3489\n",
      "Epoch [5/20], Loss: 0.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 6/20 [41:48<1:37:32, 418.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 7/20 [48:44<1:30:22, 417.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.3197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 8/20 [55:40<1:23:23, 416.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 9/20 [1:02:38<1:16:27, 417.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.3240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 11/20 [1:16:33<1:02:36, 417.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 12/20 [1:23:30<55:38, 417.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 13/20 [1:30:27<48:40, 417.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 14/20 [1:37:25<41:43, 417.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.3129\n",
      "Epoch [15/20], Loss: 0.3219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 16/20 [1:51:22<27:51, 417.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 17/20 [1:58:20<20:54, 418.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 18/20 [2:05:18<13:55, 417.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 19/20 [2:12:15<06:57, 417.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.3096\n",
      "Epoch [20/20], Loss: 0.3138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 20/20 [2:19:14<00:00, 417.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved to vqmodel_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 设备设置\n",
    "model.to(device)\n",
    "\n",
    "# 开始训练\n",
    "num_epochs = 20\n",
    "train(model, train_loader, loss_fn, optimizer, num_epochs)\n",
    "\n",
    "# 保存模型参数为 ckpt 文件\n",
    "torch.save(model.state_dict(), 'vqmodel_checkpoint.ckpt')\n",
    "print(\"Model parameters saved to vqmodel_checkpoint.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
